{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b6ba85-b7d2-4eab-a65a-124c2c9c1d28",
   "metadata": {},
   "source": [
    "# BDH Legal Violation Classifier — Notebook B (Fixed)\n",
    "### Kaggle Dual-T4 · Top-8 ECHR Articles · All Fixes Applied\n",
    "\n",
    "| Fix | What changed | Expected gain |\n",
    "|-----|-------------|---------------|\n",
    "| **Bidirectional attention** | Removed `tril()` mask — encoder now sees full context | +5–10 F1 |\n",
    "| **Hierarchical chunking** | Split long docs into chunks, pool chunk embeddings | +3–8 F1 |\n",
    "| **`pos_weight` restored** | BCE re-weighted per label for class imbalance | +2–4 F1 |\n",
    "| **Per-label threshold** | Optimal threshold found per article on val set | +1–3 F1 |\n",
    "| **Dual-GPU via DataParallel** | Both Kaggle T4s used automatically | ~2× throughput |\n",
    "| **Tokenizer parallelism fix** | `TOKENIZERS_PARALLELISM=false` before any fork | No more warning |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d75b68-3fdb-43a2-ac3f-5334264561e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /venv/main/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /venv/main/lib/python3.12/site-packages (from pandas) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba02d99-3b21-4d4b-8354-df092cd98727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: tqdm in /venv/main/lib/python3.12/site-packages (4.67.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /venv/main/lib/python3.12/site-packages (from scikit-learn) (2.4.1)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /venv/main/lib/python3.12/site-packages (from matplotlib) (12.1.0)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /venv/main/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
      "Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, pyparsing, kiwisolver, joblib, fonttools, cycler, contourpy, scikit-learn, matplotlib\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/10\u001b[0m [matplotlib]0\u001b[0m [matplotlib]n]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 joblib-1.5.3 kiwisolver-1.4.9 matplotlib-3.10.8 pyparsing-3.3.2 scikit-learn-1.8.0 scipy-1.17.0 threadpoolctl-3.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ca8183-4970-42a0-be50-fc6247572381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers>=4.40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7534a194-04b1-4415-9dbe-d9451da8d772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENIZERS_PARALLELISM = false\n"
     ]
    }
   ],
   "source": [
    "#  1. ENVIRONMENT SETUP \n",
    "import os\n",
    "\n",
    "# Fix tokenizer parallelism warning  must be set BEFORE importing tokenizers\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "print(f\"TOKENIZERS_PARALLELISM = {os.environ['TOKENIZERS_PARALLELISM']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a4831ff-199c-423a-ad71-896d8a6eb696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch : 2.10.0+cu126\n",
      "GPUs    : 1\n",
      "  GPU 0: NVIDIA A100-SXM4-40GB  (42.3 GB)\n",
      "DataParallel : False  (will use 1 GPU)\n"
     ]
    }
   ],
   "source": [
    "#  2. IMPORTS & SEED \n",
    "import json, zipfile, shutil, random, math, time, warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")          # no display needed on Kaggle\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#  Reproducibility \n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark     = False\n",
    "\n",
    "#  Dual-GPU setup \n",
    "n_gpus = torch.cuda.device_count()\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"PyTorch : {torch.__version__}\")\n",
    "print(f\"GPUs    : {n_gpus}\")\n",
    "for i in range(n_gpus):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"  GPU {i}: {props.name}  ({props.total_memory/1e9:.1f} GB)\")\n",
    "\n",
    "VRAM = torch.cuda.get_device_properties(0).total_memory / 1e9 if n_gpus > 0 else 0\n",
    "USE_DP = n_gpus > 1          # DataParallel flag  True on Kaggle dual-T4\n",
    "print(f\"DataParallel : {USE_DP}  (will use {n_gpus} GPU{'s' if n_gpus>1 else ''})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8bf635-644e-4f45-8435-866e1f5ea927",
   "metadata": {},
   "source": [
    "## Section 1 — Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebdf835f-4877-40d9-ae1f-90c13a04d9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ECHR dataset from Kaggle API...\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/mathurinache/ecthrnaacl2021...\n",
      "Downloaded: 100.0%\n",
      "✓ Downloaded to /workspace/ecthr_data/dataset.zip\n",
      "\n",
      "Extracting files...\n",
      "✓ Extraction complete!\n",
      "\n",
      "✓ Cleaned up zip file\n",
      "\n",
      "Verifying dataset...\n",
      "✓ JSONL/JSON files: 3\n",
      "✓ Parquet files: 0\n",
      "\n",
      "Dataset files found:\n",
      "  - dev.jsonl (11.06 MB)\n",
      "  - test.jsonl (11.97 MB)\n",
      "  - train.jsonl (90.29 MB)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "DATA_ROOT = Path(\"/workspace/ecthr_data\")\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Downloading ECHR dataset from Kaggle API...\")\n",
    "\n",
    "# Kaggle API endpoint for dataset download\n",
    "dataset_name = \"mathurinache/ecthrnaacl2021\"\n",
    "api_key = os.environ.get(\"KAGGLE_API_KEY\", \"\")\n",
    "username = os.environ.get(\"KAGGLE_USERNAME\", \"\")\n",
    "\n",
    "# Create auth tuple\n",
    "auth = (username, api_key)\n",
    "\n",
    "# Kaggle API URL\n",
    "url = f\"https://www.kaggle.com/api/v1/datasets/download/{dataset_name}\"\n",
    "\n",
    "try:\n",
    "    print(f\"Downloading from {url}...\")\n",
    "    response = requests.get(url, auth=auth, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        zip_path = DATA_ROOT / \"dataset.zip\"\n",
    "        \n",
    "        # Download with progress\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        downloaded = 0\n",
    "        \n",
    "        with open(zip_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    downloaded += len(chunk)\n",
    "                    if total_size:\n",
    "                        percent = (downloaded / total_size) * 100\n",
    "                        print(f\"Downloaded: {percent:.1f}%\", end='\\r')\n",
    "        \n",
    "        print(f\"\\n Downloaded to {zip_path}\\n\")\n",
    "        \n",
    "        # Extract\n",
    "        print(\"Extracting files...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "            zf.extractall(DATA_ROOT)\n",
    "        \n",
    "        print(\" Extraction complete!\\n\")\n",
    "        \n",
    "        # Clean up zip\n",
    "        os.remove(zip_path)\n",
    "        print(\" Cleaned up zip file\\n\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Verify\n",
    "print(\"Verifying dataset...\")\n",
    "def find_files(root, patterns):\n",
    "    found = []\n",
    "    for pat in patterns:\n",
    "        found.extend(sorted(root.rglob(pat)))\n",
    "    return found\n",
    "\n",
    "jsonl_files = find_files(DATA_ROOT, [\"*.jsonl\", \"*.json\"])\n",
    "parquet_files = find_files(DATA_ROOT, [\"*.parquet\"])\n",
    "\n",
    "print(f\" JSONL/JSON files: {len(jsonl_files)}\")\n",
    "print(f\" Parquet files: {len(parquet_files)}\")\n",
    "\n",
    "if jsonl_files:\n",
    "    print(\"\\nDataset files found:\")\n",
    "    for f in jsonl_files[:10]:\n",
    "        size_mb = f.stat().st_size / (1024*1024)\n",
    "        print(f\"  - {f.name} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8075d3e7-e6a2-4b83-ba4e-c4878392d6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading splits …\n",
      "  ✓ train → JSONL: train.jsonl  (92456 KB)\n",
      "  ✓ dev → JSONL: dev.jsonl  (11324 KB)\n",
      "  ✓ test → JSONL: test.jsonl  (12257 KB)\n",
      "train=9,000  val=1,000  test=1,000\n",
      "Sample keys: ['case_id', 'case_no', 'title', 'judgment_date', 'facts', 'applicants', 'defendants', 'allegedly_violated_articles', 'violated_articles', 'court_assessment_references', 'silver_rationales', 'gold_rationales']\n"
     ]
    }
   ],
   "source": [
    "#  4. LOAD SPLITS \n",
    "\n",
    "def load_jsonl(path: Path):\n",
    "    rows = []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def load_split(keywords: List[str]):\n",
    "    for kw in keywords:\n",
    "        for f in jsonl_files:\n",
    "            if kw in f.name.lower():\n",
    "                print(f\"   {kw}  JSONL: {f.name}  ({f.stat().st_size/1024:.0f} KB)\")\n",
    "                return load_jsonl(f)\n",
    "        for f in parquet_files:\n",
    "            if kw in f.name.lower():\n",
    "                print(f\"   {kw}  Parquet: {f.name}\")\n",
    "                return pd.read_parquet(f).to_dict(\"records\")\n",
    "    print(f\"   No file matched for keywords: {keywords}\")\n",
    "    return []\n",
    "\n",
    "print(\"Loading splits \")\n",
    "raw_train = load_split([\"train\"])\n",
    "raw_val   = load_split([\"dev\", \"val\", \"valid\", \"validation\"])\n",
    "raw_test  = load_split([\"test\"])\n",
    "\n",
    "print(f\"train={len(raw_train):,}  val={len(raw_val):,}  test={len(raw_test):,}\")\n",
    "if raw_train:\n",
    "    print(f\"Sample keys: {list(raw_train[0].keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a03b420-dd45-4ab6-9274-a43f2e5fa60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-7 ECHR articles selected (Art 13 Excluded):\n",
      "Article         Train count      %\n",
      "------------------------------------\n",
      "  Art 10                  291    3.2%\n",
      "  Art 2                   505    5.6%\n",
      "  Art 3                 1,349   15.0%\n",
      "  Art 5                 1,368   15.2%\n",
      "  Art 6                 4,704   52.3%\n",
      "  Art 8                   710    7.9%\n",
      "  Art P1-1              1,421   15.8%\n",
      "label2idx = {'10': 0, '2': 1, '3': 2, '5': 3, '6': 4, '8': 5, 'P1-1': 6}\n"
     ]
    }
   ],
   "source": [
    "#  5. TOP-8 LABEL MAP \n",
    "\n",
    "def get_articles(rec) -> List[str]:\n",
    "    for key in (\"violated_articles\", \"labels\", \"violated_provisions\",\n",
    "                \"gold_labels\", \"silver_rationales\"):\n",
    "        val = rec.get(key)\n",
    "        if val is not None:\n",
    "            if isinstance(val, list):\n",
    "                return [str(v).strip() for v in val if v is not None and str(v).strip()]\n",
    "            if isinstance(val, str) and val.strip():\n",
    "                return [val.strip()]\n",
    "    return []\n",
    "\n",
    "def get_text(rec) -> str:\n",
    "    for key in (\"facts\", \"text\", \"case_text\", \"content\"):\n",
    "        val = rec.get(key)\n",
    "        if val is not None:\n",
    "            if isinstance(val, list):\n",
    "                return \" \".join(str(v) for v in val if v)\n",
    "            return str(val)\n",
    "    # Fallback: longest string values\n",
    "    parts = [str(v) for v in rec.values() if isinstance(v, str) and len(v) > 30]\n",
    "    return \" \".join(parts)\n",
    "\n",
    "# Count label frequencies\n",
    "all_labels_raw = []\n",
    "for rec in raw_train:\n",
    "    all_labels_raw.extend(get_articles(rec))\n",
    "label_counts = Counter(all_labels_raw)\n",
    "if '13' in label_counts:\n",
    "    del label_counts['13']\n",
    " \n",
    "\n",
    "TOP_K      = 7\n",
    "\n",
    "top_labels = [lbl for lbl, _ in label_counts.most_common(TOP_K)]\n",
    "top_labels.sort()                # keep consistent alphabetical order\n",
    "NUM_LABELS = len(top_labels)\n",
    "label2idx  = {lbl: i for i, lbl in enumerate(top_labels)}\n",
    "idx2label  = {i: lbl for lbl, i in label2idx.items()}\n",
    "\n",
    "print(f\"Top-{TOP_K} ECHR articles selected (Art 13 Excluded):\")\n",
    "print(f\"{'Article':<14} {'Train count':>12} {'%':>6}\")\n",
    "print(\"-\" * 36)\n",
    "for lbl in top_labels:\n",
    "    cnt = label_counts[lbl]\n",
    "    print(f\"  Art {lbl:<10s} {cnt:>12,} {100*cnt/len(raw_train):>6.1f}%\")\n",
    "print(f\"label2idx = {label2idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2598bfe8-0bf9-43f1-9632-031c77dc928b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37c18701-2978-41ef-8253-520ea1ca9f78",
   "metadata": {},
   "source": [
    "## Section 2 — Tokenizer & Hierarchical Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35801f33-0bd8-4ee8-826c-fd04e0d5e6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer : bert-base-uncased\n",
      "Vocab     : 30,522\n",
      "Chunk size: 512 tokens  (overlap=64)\n",
      "Max chunks: 8  → max doc tokens seen = 3,584\n",
      "Sample : 'The applicant alleged a violation of Article 6 of the Convention.'\n",
      "Tokens : ['[CLS]', 'the', 'applicant', 'alleged', 'a', 'violation', 'of', 'article', '6', 'of', 'the', 'convention', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "#  6. TOKENIZER \n",
    "# TOKENIZERS_PARALLELISM is already set to false before any imports,\n",
    "# so num_workers > 0 in DataLoader will not cause the fork warning.\n",
    "\n",
    "TOKENIZER_NAME = \"bert-base-uncased\"   # vocab = 30,522\n",
    "CHUNK_SIZE     = 512                   # tokens per chunk\n",
    "CHUNK_OVERLAP  = 64                    # overlapping tokens between chunks\n",
    "MAX_CHUNKS     = 8                     # max chunks per document\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "print(f\"Tokenizer : {TOKENIZER_NAME}\")\n",
    "print(f\"Vocab     : {tokenizer.vocab_size:,}\")\n",
    "print(f\"Chunk size: {CHUNK_SIZE} tokens  (overlap={CHUNK_OVERLAP})\")\n",
    "print(f\"Max chunks: {MAX_CHUNKS}   max doc tokens seen = {MAX_CHUNKS * (CHUNK_SIZE - CHUNK_OVERLAP):,}\")\n",
    "\n",
    "# Quick sanity check\n",
    "sample = \"The applicant alleged a violation of Article 6 of the Convention.\"\n",
    "enc    = tokenizer(sample, return_tensors=\"pt\")\n",
    "print(f\"Sample : '{sample}'\")\n",
    "print(f\"Tokens : {tokenizer.convert_ids_to_tokens(enc['input_ids'][0].tolist())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "804d6c00-5e12-4273-8b45-3aa0f4a3ae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1602 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo document: 1602 tokens\n",
      "Chunks produced: 4 × 512 tokens each\n",
      "  → 2048 tokens total seen by model\n"
     ]
    }
   ],
   "source": [
    "#  7. HIERARCHICAL CHUNKING \n",
    "# Strategy: tokenise the full text, then slide a window of CHUNK_SIZE tokens\n",
    "# with CHUNK_OVERLAP overlap. Each chunk is padded to CHUNK_SIZE.\n",
    "# At inference, BDH encodes each chunk independently, then we mean-pool the\n",
    "# chunk [CLS-equivalent] representations before the classification head.\n",
    "\n",
    "def text_to_chunks(text: str,\n",
    "                   tokenizer,\n",
    "                   chunk_size: int = CHUNK_SIZE,\n",
    "                   overlap: int    = CHUNK_OVERLAP,\n",
    "                   max_chunks: int = MAX_CHUNKS):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        input_ids      : (n_chunks, chunk_size)  LongTensor\n",
    "        attention_masks: (n_chunks, chunk_size)  LongTensor\n",
    "    \"\"\"\n",
    "    # Tokenize full text without truncation\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,   # [CLS] ... [SEP]\n",
    "        truncation=False,\n",
    "        return_attention_mask=False,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    stride = chunk_size - overlap\n",
    "    chunks_ids, chunks_mask = [], []\n",
    "\n",
    "    start = 0\n",
    "    while start < len(tokens) and len(chunks_ids) < max_chunks:\n",
    "        end   = min(start + chunk_size, len(tokens))\n",
    "        chunk = tokens[start:end]\n",
    "\n",
    "        # Pad if shorter than chunk_size\n",
    "        pad_len  = chunk_size - len(chunk)\n",
    "        chunk    = chunk + [tokenizer.pad_token_id] * pad_len\n",
    "        mask     = [1] * (len(tokens[start:end])) + [0] * pad_len\n",
    "\n",
    "        chunks_ids.append(chunk)\n",
    "        chunks_mask.append(mask)\n",
    "        start += stride\n",
    "\n",
    "    return (\n",
    "        torch.tensor(chunks_ids,  dtype=torch.long),   # (n_chunks, chunk_size)\n",
    "        torch.tensor(chunks_mask, dtype=torch.long),   # (n_chunks, chunk_size)\n",
    "    )\n",
    "\n",
    "#  Verify on a realistic document \n",
    "demo_text = \" \".join([\"The applicant alleged violations of the Convention.\"] * 200)\n",
    "ids, masks = text_to_chunks(demo_text, tokenizer)\n",
    "print(f\"Demo document: {len(tokenizer(demo_text, truncation=False)['input_ids'])} tokens\")\n",
    "print(f\"Chunks produced: {ids.shape[0]} × {ids.shape[1]} tokens each\")\n",
    "print(f\"   {ids.shape[0] * ids.shape[1]} tokens total seen by model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fad8a225-a2ca-480d-99e1-1970dd8de144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing splits …\n",
      "  train: 7,968 kept | skip_no_top8=1032 | skip_empty=0\n",
      "  val: 813 kept | skip_no_top8=187 | skip_empty=0\n",
      "  test: 822 kept | skip_no_top8=178 | skip_empty=0\n",
      "Token lengths (300 train docs):\n",
      "  mean=1776  median=1142  p95=4937  max=22637\n",
      "  → docs needing >1 chunk: 211 / 300\n"
     ]
    }
   ],
   "source": [
    "#  8. DATASET: PROCESS RECORDS (TOP-8 FILTER) \n",
    "\n",
    "def encode_labels(articles: List[str]) -> List[float]:\n",
    "    vec = [0.0] * NUM_LABELS\n",
    "    for art in articles:\n",
    "        if art in label2idx:\n",
    "            vec[label2idx[art]] = 1.0\n",
    "    return vec\n",
    "\n",
    "def process_records(records, split_name=\"\"):\n",
    "    texts, labels = [], []\n",
    "    skip_no_top8, skip_empty = 0, 0\n",
    "    for rec in records:\n",
    "        t    = get_text(rec)\n",
    "        arts = get_articles(rec)\n",
    "        if not any(a in label2idx for a in arts):\n",
    "            skip_no_top8 += 1\n",
    "            continue\n",
    "        if len(t.strip()) < 20:\n",
    "            skip_empty += 1\n",
    "            continue\n",
    "        texts.append(t)\n",
    "        labels.append(encode_labels(arts))\n",
    "    if split_name:\n",
    "        print(f\"  {split_name}: {len(texts):,} kept | \"\n",
    "              f\"skip_no_top8={skip_no_top8} | skip_empty={skip_empty}\")\n",
    "    return texts, labels\n",
    "\n",
    "print(\"Processing splits \")\n",
    "train_texts, train_labels = process_records(raw_train, \"train\")\n",
    "val_texts,   val_labels   = process_records(raw_val,   \"val\")\n",
    "test_texts,  test_labels  = process_records(raw_test,  \"test\")\n",
    "\n",
    "#  Token-length stats (sample) \n",
    "sample_n   = min(300, len(train_texts))\n",
    "sample_len = [\n",
    "    len(tokenizer(t, truncation=False)[\"input_ids\"])\n",
    "    for t in train_texts[:sample_n]\n",
    "]\n",
    "print(f\"Token lengths ({sample_n} train docs):\")\n",
    "print(f\"  mean={np.mean(sample_len):.0f}  median={np.median(sample_len):.0f}\"\n",
    "      f\"  p95={np.percentile(sample_len,95):.0f}  max={max(sample_len)}\")\n",
    "print(f\"   docs needing >1 chunk: \"\n",
    "      f\"{sum(l > CHUNK_SIZE for l in sample_len)} / {sample_n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c01909d-2565-40ba-9dcb-0287ad624b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building datasets …\n",
      "  Pre-tokenizing 7,968 documents …\n",
      "  Done.\n",
      "  Pre-tokenizing 813 documents …\n",
      "  Done.\n",
      "  Pre-tokenizing 822 documents …\n",
      "  Done.\n",
      "DataLoaders (batch=4):\n",
      "  train=1992 batches | val=204 | test=206\n"
     ]
    }
   ],
   "source": [
    "#  9. CHUNKED DATASET \n",
    "\n",
    "class ECHRChunkedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pre-computes chunk tensors at construction time (once, on CPU).\n",
    "    Each item: dict with\n",
    "        chunk_ids   : (n_chunks, CHUNK_SIZE)\n",
    "        chunk_masks : (n_chunks, CHUNK_SIZE)\n",
    "        n_chunks    : int\n",
    "        labels      : (NUM_LABELS,)\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer,\n",
    "                 chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP, max_chunks=MAX_CHUNKS):\n",
    "        self.labels     = labels\n",
    "        self.chunk_size = chunk_size\n",
    "        self.max_chunks = max_chunks\n",
    "\n",
    "        print(f\"  Pre-tokenizing {len(texts):,} documents \")\n",
    "        self.chunks_ids  = []\n",
    "        self.chunks_mask = []\n",
    "        for text in texts:\n",
    "            ids, msk = text_to_chunks(text, tokenizer, chunk_size, overlap, max_chunks)\n",
    "            self.chunks_ids.append(ids)\n",
    "            self.chunks_mask.append(msk)\n",
    "        print(f\"  Done.\")\n",
    "\n",
    "    def __len__(self): return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"chunk_ids\"  : self.chunks_ids[idx],   # (n_chunks, chunk_size)\n",
    "            \"chunk_masks\": self.chunks_mask[idx],   # (n_chunks, chunk_size)\n",
    "            \"n_chunks\"   : self.chunks_ids[idx].shape[0],\n",
    "            \"labels\"     : torch.tensor(self.labels[idx], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "def collate_chunks(batch):\n",
    "    \"\"\"\n",
    "    Pad batch to the same number of chunks (max in this batch).\n",
    "    Output shapes:\n",
    "        chunk_ids   : (B, max_n_chunks, CHUNK_SIZE)\n",
    "        chunk_masks : (B, max_n_chunks, CHUNK_SIZE)\n",
    "        doc_masks   : (B, max_n_chunks)   which chunk slots are real vs padding\n",
    "        labels      : (B, NUM_LABELS)\n",
    "    \"\"\"\n",
    "    max_nc = max(item[\"n_chunks\"] for item in batch)\n",
    "    B      = len(batch)\n",
    "    C      = batch[0][\"chunk_ids\"].shape[1]\n",
    "\n",
    "    chunk_ids_pad   = torch.zeros(B, max_nc, C, dtype=torch.long)\n",
    "    chunk_masks_pad = torch.zeros(B, max_nc, C, dtype=torch.long)\n",
    "    doc_masks       = torch.zeros(B, max_nc, dtype=torch.bool)\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        nc = item[\"n_chunks\"]\n",
    "        chunk_ids_pad[i,   :nc] = item[\"chunk_ids\"]\n",
    "        chunk_masks_pad[i, :nc] = item[\"chunk_masks\"]\n",
    "        doc_masks[i,       :nc] = True\n",
    "\n",
    "    return {\n",
    "        \"chunk_ids\"  : chunk_ids_pad,\n",
    "        \"chunk_masks\": chunk_masks_pad,\n",
    "        \"doc_masks\"  : doc_masks,\n",
    "        \"labels\"     : torch.stack([item[\"labels\"] for item in batch]),\n",
    "    }\n",
    "\n",
    "print(\"Building datasets \")\n",
    "train_ds = ECHRChunkedDataset(train_texts, train_labels, tokenizer)\n",
    "val_ds   = ECHRChunkedDataset(val_texts,   val_labels,   tokenizer)\n",
    "test_ds  = ECHRChunkedDataset(test_texts,  test_labels,  tokenizer)\n",
    "\n",
    "# Batch size: larger is fine now (each forward pass is per-chunk, not per-doc)\n",
    "BATCH_SIZE = 4 if VRAM >= 14 else 8\n",
    "\n",
    "# num_workers=0 avoids tokenizer fork issues (we pre-tokenized anyway)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=0, collate_fn=collate_chunks, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=0, collate_fn=collate_chunks, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=0, collate_fn=collate_chunks, pin_memory=True)\n",
    "\n",
    "print(f\"DataLoaders (batch={BATCH_SIZE}):\")\n",
    "print(f\"  train={len(train_loader)} batches | val={len(val_loader)} | test={len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09503e63-bcae-42b8-9ba2-1dc17f91a201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./label_dist.png\n"
     ]
    }
   ],
   "source": [
    "#  10. LABEL DISTRIBUTION \n",
    "label_freq_np = np.array(train_labels).sum(0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "bars = ax.bar(range(NUM_LABELS), label_freq_np,\n",
    "              color=plt.cm.tab10(np.arange(NUM_LABELS)))\n",
    "ax.set_xticks(range(NUM_LABELS))\n",
    "ax.set_xticklabels([f\"Art {top_labels[i]}\" for i in range(NUM_LABELS)], fontsize=11)\n",
    "ax.set_ylabel(\"Training examples\")\n",
    "ax.set_title(f\"Top-{NUM_LABELS} ECHR articles  train set\")\n",
    "for i, v in enumerate(label_freq_np):\n",
    "    ax.text(i, v + 30, f\"{int(v)}\", ha=\"center\", fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./label_dist.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: ./label_dist.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5f059-c6d0-4f24-8ef1-0ffe28c69de9",
   "metadata": {},
   "source": [
    "## Section 3 — BDH Architecture (Bidirectional, Fixed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4d0ed8-3521-444f-ad3c-28fcc17bd5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  11. BDH ARCHITECTURE \n",
    "\n",
    "@dataclass\n",
    "class BDHConfig:\n",
    "    n_layer  : int   = 6        # shared weight layers (Universal Transformer)\n",
    "    n_embd   : int   = 256      # D: hidden dimension\n",
    "    n_head   : int   = 4        # nh: attention heads\n",
    "    mlp_internal_dim_multiplier: int = 64  # N = 64*256/4 = 4096\n",
    "    dropout  : float = 0.15\n",
    "    vocab_size : int = 30522    # bert-base-uncased\n",
    "    num_labels : int = 8        # set at runtime\n",
    "\n",
    "#  RoPE frequencies \n",
    "def get_freqs(n: int, theta: int, dtype) -> torch.Tensor:\n",
    "    def quantize(t, q=2): return (t / q).floor() * q\n",
    "    return (1.0 / (theta ** (quantize(torch.arange(0, n, 1, dtype=dtype)) / n))\n",
    "            / (2 * math.pi))\n",
    "\n",
    "#  BDH Attention  BIDIRECTIONAL\n",
    "class BDHAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    BDH self-attention with RoPE.\n",
    "\n",
    "    KEY FIX vs previous versions:\n",
    "      The causal tril() mask is REMOVED.\n",
    "      This is a classification encoder  all tokens must attend to all others.\n",
    "      The tril mask was silently destroying contextual representations.\n",
    "\n",
    "    NaN safety:\n",
    "      - scores scaled by 1/sqrt(N) before softmax\n",
    "      - all ops in fp32 regardless of AMP autocast\n",
    "    \"\"\"\n",
    "    def __init__(self, config: BDHConfig):\n",
    "        super().__init__()\n",
    "        nh = config.n_head\n",
    "        D  = config.n_embd\n",
    "        N  = config.mlp_internal_dim_multiplier * D // nh\n",
    "        self.N = N\n",
    "        self.register_buffer(\n",
    "            \"freqs\",\n",
    "            get_freqs(N, theta=2**16, dtype=torch.float32).view(1, 1, 1, N),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _rope(phases: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
    "        v_rot = torch.stack((-v[..., 1::2], v[..., ::2]), dim=-1).view(*v.shape)\n",
    "        ph = (phases % 1) * (2 * math.pi)\n",
    "        return v * torch.cos(ph) + v_rot * torch.sin(ph)\n",
    "\n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "        assert K is Q, \"BDH requires K is Q (self-attention)\"\n",
    "        _, _, T, _ = Q.shape\n",
    "\n",
    "        # Force fp32  prevents fp16 overflow on large N\n",
    "        Q32 = Q.float()\n",
    "        V32 = V.float()\n",
    "\n",
    "        r_phases = (\n",
    "            torch.arange(T, device=self.freqs.device, dtype=torch.float32)\n",
    "            .view(1, 1, -1, 1)\n",
    "        ) * self.freqs\n",
    "\n",
    "        QR = self._rope(r_phases, Q32)              # (B, nh, T, N)\n",
    "\n",
    "        #  BIDIRECTIONAL: no causal mask \n",
    "        scale  = 1.0 / math.sqrt(self.N)\n",
    "        scores = (QR @ QR.mT) * scale               # (B, nh, T, T)\n",
    "        # scores = scores.tril(diagonal=-1)          #  REMOVED (was the bug)\n",
    "\n",
    "        out = scores @ V32                           # (B, nh, T, D)\n",
    "        return out.to(Q.dtype)\n",
    "\n",
    "\n",
    "#  BDH Backbone \n",
    "class BDHBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared-weight BDH encoder. Universal Transformer style:\n",
    "    same encoder/encoder_v/decoder params reused across all L layers.\n",
    "\n",
    "    Param budget (L=6, D=256, nh=4, N=4096):\n",
    "        encoder   (nh, D, N): 4*256*4096 = 4,194,304\n",
    "        encoder_v (nh, D, N): 4,194,304\n",
    "        decoder (nh*N, D)   : 4,194,304\n",
    "        embed   (vocab, D)  : 7,813,632\n",
    "        Total backbone      : ~20.4M\n",
    "    \"\"\"\n",
    "    def __init__(self, config: BDHConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        nh = config.n_head\n",
    "        D  = config.n_embd\n",
    "        N  = config.mlp_internal_dim_multiplier * D // nh\n",
    "\n",
    "        # Shared weights  used identically at each of the L layers\n",
    "        self.encoder   = nn.Parameter(torch.empty(nh, D, N).normal_(std=0.02))\n",
    "        self.encoder_v = nn.Parameter(torch.empty(nh, D, N).normal_(std=0.02))\n",
    "        self.decoder   = nn.Parameter(torch.empty(nh * N, D).normal_(std=0.02))\n",
    "\n",
    "        self.attn  = BDHAttention(config)\n",
    "        self.ln    = nn.LayerNorm(D, elementwise_affine=False, bias=False)\n",
    "        self.embed = nn.Embedding(config.vocab_size, D)\n",
    "        self.drop  = nn.Dropout(config.dropout)\n",
    "\n",
    "        nn.init.normal_(self.embed.weight, std=0.02)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args: idx (B, T)\n",
    "        Returns: hidden (B, T, D)\n",
    "        \"\"\"\n",
    "        C  = self.config\n",
    "        B, T = idx.shape\n",
    "        nh = C.n_head\n",
    "        N  = C.mlp_internal_dim_multiplier * C.n_embd // nh\n",
    "\n",
    "        x = self.embed(idx).unsqueeze(1)    # (B, 1, T, D)\n",
    "        x = self.ln(x)\n",
    "\n",
    "        for _ in range(C.n_layer):\n",
    "            # Project to sparse N-dim basis\n",
    "            x_latent  = x @ self.encoder        # (B, nh, T, N)\n",
    "            x_sparse  = F.relu(x_latent)        # sparse activations\n",
    "\n",
    "            # BDH attention (bidirectional)\n",
    "            yKV       = self.attn(Q=x_sparse, K=x_sparse, V=x)\n",
    "            yKV       = self.ln(yKV)\n",
    "\n",
    "            # Value encoder\n",
    "            y_latent  = yKV @ self.encoder_v    # (B, nh, T, N)\n",
    "            y_sparse  = F.relu(y_latent)\n",
    "\n",
    "            # Hebbian product  pre × post synaptic correlation\n",
    "            xy_sparse = x_sparse * y_sparse     # (B, nh, T, N)\n",
    "            xy_sparse = self.drop(xy_sparse)\n",
    "\n",
    "            # Decode back to D-dim\n",
    "            yMLP = (\n",
    "                xy_sparse.transpose(1, 2)            # (B, T, nh, N)\n",
    "                .reshape(B, 1, T, nh * N)\n",
    "                @ self.decoder                       # (B, 1, T, D)\n",
    "            )\n",
    "            y = self.ln(yMLP)\n",
    "            x = self.ln(x + y)                   # residual\n",
    "\n",
    "        return x.squeeze(1)                      # (B, T, D)\n",
    "\n",
    "\n",
    "#  Hierarchical BDH Classifier \n",
    "class BDHClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Hierarchical BDH:\n",
    "      1. Run BDHBackbone on each chunk independently  (B, n_chunks, T, D)\n",
    "      2. Mean-pool each chunk's tokens  (B, n_chunks, D)   [chunk representations]\n",
    "      3. Mean-pool valid chunks (using doc_mask)  (B, D)   [document representation]\n",
    "      4. LN  Linear head  (B, num_labels)\n",
    "    \"\"\"\n",
    "    def __init__(self, config: BDHConfig):\n",
    "        super().__init__()\n",
    "        self.backbone = BDHBackbone(config)\n",
    "        self.ln_post  = nn.LayerNorm(config.n_embd, elementwise_affine=False, bias=False)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.n_embd, config.num_labels),\n",
    "        )\n",
    "        for m in self.head.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    @staticmethod\n",
    "    def _token_pool(hidden: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Masked mean-pool over tokens. hidden: (B,T,D), mask: (B,T)  (B,D)\"\"\"\n",
    "        m = mask.unsqueeze(-1).float()\n",
    "        return (hidden * m).sum(1) / m.sum(1).clamp(min=1e-9)\n",
    "\n",
    "    def forward(self,\n",
    "                chunk_ids   : torch.Tensor,    # (B, n_chunks, T)\n",
    "                chunk_masks : torch.Tensor,    # (B, n_chunks, T)\n",
    "                doc_masks   : torch.Tensor,    # (B, n_chunks) bool\n",
    "               ) -> torch.Tensor:              # (B, num_labels)\n",
    "        B, n_chunks, T = chunk_ids.shape\n",
    "\n",
    "        #  Encode all chunks \n",
    "        # Flatten to (B*n_chunks, T) for a single backbone pass\n",
    "        ids_flat   = chunk_ids.view(B * n_chunks, T)\n",
    "        masks_flat = chunk_masks.view(B * n_chunks, T)\n",
    "\n",
    "        hidden_flat = self.backbone(ids_flat)                   # (B*n_chunks, T, D)\n",
    "\n",
    "        # Mean-pool tokens within each chunk  chunk representation\n",
    "        chunk_repr  = self._token_pool(hidden_flat, masks_flat) # (B*n_chunks, D)\n",
    "        chunk_repr  = chunk_repr.view(B, n_chunks, -1)          # (B, n_chunks, D)\n",
    "\n",
    "        #  Pool across valid chunks \n",
    "        dm = doc_masks.unsqueeze(-1).float()                    # (B, n_chunks, 1)\n",
    "        doc_repr = (chunk_repr * dm).sum(1) / dm.sum(1).clamp(min=1e-9)  # (B, D)\n",
    "\n",
    "        doc_repr = self.ln_post(doc_repr)\n",
    "        return self.head(doc_repr)                              # (B, num_labels)\n",
    "\n",
    "    def count_parameters(self):\n",
    "        total = trainable = 0\n",
    "        for p in self.parameters():\n",
    "            total += p.numel()\n",
    "            if p.requires_grad: trainable += p.numel()\n",
    "        return total, trainable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba06ba2c-2ffa-49ea-96b5-b6df5be3418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "══════════════════════════════════════════════════════════════\n",
      "BDH CLASSIFIER — PARAMETER BUDGET\n",
      "══════════════════════════════════════════════════════════════\n",
      "  D (hidden)           : 256\n",
      "  nh (heads)           : 4\n",
      "  N (sparse dim)       : 4096\n",
      "  Layers L (shared)    : 6\n",
      "  Dropout              : 0.15\n",
      "  Vocab                : 30,522\n",
      "  Labels               : 7\n",
      "  Chunk size           : 512 × 8 max chunks\n",
      "──────────────────────────────────────────────────────────────\n",
      "  encoder + encoder_v  :    8,388,608\n",
      "  decoder              :    4,194,304\n",
      "  embedding            :    7,813,632\n",
      "  head                 :       67,584\n",
      "──────────────────────────────────────────────────────────────\n",
      "  TOTAL                :   20,464,135  (20.46 M)\n",
      "══════════════════════════════════════════════════════════════\n",
      "  Forward pass  → (2, 7)  ✓\n",
      "  NaN check     → clean  ✓\n",
      "══════════════════════════════════════════════════════════════\n"
     ]
    }
   ],
   "source": [
    "#  12. INSTANTIATE MODEL \n",
    "\n",
    "CFG = BDHConfig(\n",
    "    n_layer  = 6,\n",
    "    n_embd   = 256,\n",
    "    n_head   = 4,\n",
    "    mlp_internal_dim_multiplier = 64,   # N = 64*256/4 = 4096\n",
    "    dropout  = 0.15,\n",
    "    vocab_size  = tokenizer.vocab_size,\n",
    "    num_labels  = NUM_LABELS,\n",
    ")\n",
    "\n",
    "model = BDHClassifier(CFG).to(DEVICE)\n",
    "\n",
    "#  Wrap with DataParallel if dual-GPU \n",
    "if USE_DP:\n",
    "    model = nn.DataParallel(model)\n",
    "    print(f\" DataParallel enabled across {n_gpus} GPUs\")\n",
    "\n",
    "# Count params (unwrap DP for counting)\n",
    "core_model = model.module if USE_DP else model\n",
    "total_p, train_p = core_model.count_parameters()\n",
    "\n",
    "nh = CFG.n_head; D = CFG.n_embd\n",
    "N  = CFG.mlp_internal_dim_multiplier * D // nh\n",
    "\n",
    "print(\"\" * 62)\n",
    "print(\"BDH CLASSIFIER  PARAMETER BUDGET\")\n",
    "print(\"\" * 62)\n",
    "print(f\"  D (hidden)           : {D}\")\n",
    "print(f\"  nh (heads)           : {nh}\")\n",
    "print(f\"  N (sparse dim)       : {N}\")\n",
    "print(f\"  Layers L (shared)    : {CFG.n_layer}\")\n",
    "print(f\"  Dropout              : {CFG.dropout}\")\n",
    "print(f\"  Vocab                : {CFG.vocab_size:,}\")\n",
    "print(f\"  Labels               : {CFG.num_labels}\")\n",
    "print(f\"  Chunk size           : {CHUNK_SIZE} × {MAX_CHUNKS} max chunks\")\n",
    "print(\"\" * 62)\n",
    "enc_p = nh * D * N\n",
    "emb_p = CFG.vocab_size * D\n",
    "hd_p  = D * D + D + D * CFG.num_labels\n",
    "print(f\"  encoder + encoder_v  : {enc_p*2:>12,}\")\n",
    "print(f\"  decoder              : {enc_p:>12,}\")\n",
    "print(f\"  embedding            : {emb_p:>12,}\")\n",
    "print(f\"  head                 : {hd_p:>12,}\")\n",
    "print(\"\" * 62)\n",
    "print(f\"  TOTAL                : {total_p:>12,}  ({total_p/1e6:.2f} M)\")\n",
    "print(\"\" * 62)\n",
    "\n",
    "#  Shape / NaN check \n",
    "dummy_ids   = torch.zeros(2, 3, 16, dtype=torch.long,  device=DEVICE)  # (B,n_chunks,T)\n",
    "dummy_masks = torch.ones( 2, 3, 16, dtype=torch.long,  device=DEVICE)\n",
    "dummy_dm    = torch.ones( 2, 3,     dtype=torch.bool,  device=DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(dummy_ids, dummy_masks, dummy_dm)\n",
    "\n",
    "assert tuple(out.shape) == (2, NUM_LABELS), f\"Unexpected shape {out.shape}\"\n",
    "assert not torch.isnan(out).any(), \"NaN in forward pass!\"\n",
    "print(f\"  Forward pass   {tuple(out.shape)}  \")\n",
    "print(f\"  NaN check      clean  \")\n",
    "print(\"\" * 62)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e30e83-6632-421c-b075-c688b9a472ba",
   "metadata": {},
   "source": [
    "## Section 4 — Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd7372e7-6ddc-4f32-880e-daf13311a1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight per label:\n",
      "  Art 10        freq=  291  pw=20.000\n",
      "  Art 2         freq=  505  pw=14.778\n",
      "  Art 3         freq=1,349  pw=4.907\n",
      "  Art 5         freq=1,368  pw=4.825\n",
      "  Art 6         freq=4,704  pw=0.694\n",
      "  Art 8         freq=  710  pw=10.223\n",
      "  Art P1-1      freq=1,421  pw=4.607\n",
      "Training config:\n",
      "  Epochs=8  LR=0.0002  WD=0.05\n",
      "  Warmup=1274  TotalSteps=15,936\n",
      "  GradClip=1.0  Patience=10\n",
      "  AMP=True  Batch=4  GPUs=1\n"
     ]
    }
   ],
   "source": [
    "#  13. LOSS, OPTIMIZER, SCHEDULER \n",
    "\n",
    "#  pos_weight (restored)  corrects for class imbalance \n",
    "label_freq_t = torch.tensor(label_freq_np, dtype=torch.float)\n",
    "pos_weight   = ((len(train_labels) - label_freq_t) / label_freq_t.clamp(min=1))\n",
    "pos_weight   = pos_weight.clamp(max=20.0).to(DEVICE)\n",
    "criterion    = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction=\"mean\")\n",
    "\n",
    "print(\"pos_weight per label:\")\n",
    "for i, lbl in enumerate(top_labels):\n",
    "    print(f\"  Art {lbl:<8s}  freq={int(label_freq_np[i]):>5,}  pw={pos_weight[i].item():.3f}\")\n",
    "\n",
    "#  Hyper-parameters \n",
    "EPOCHS       = 8\n",
    "LR           = 2e-4\n",
    "WEIGHT_DECAY = 0.05\n",
    "WARMUP_FRAC  = 0.08\n",
    "GRAD_CLIP    = 1.0\n",
    "PATIENCE     = 10       # early-stopping patience (val loss)\n",
    "\n",
    "TOTAL_STEPS  = EPOCHS * len(train_loader)\n",
    "WARMUP_STEPS = int(WARMUP_FRAC * TOTAL_STEPS)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LR, betas=(0.9, 0.95), weight_decay=WEIGHT_DECAY, eps=1e-8,\n",
    ")\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < WARMUP_STEPS: return step / max(1, WARMUP_STEPS)\n",
    "    progress = (step - WARMUP_STEPS) / max(1, TOTAL_STEPS - WARMUP_STEPS)\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "USE_AMP = DEVICE.type == \"cuda\"\n",
    "scaler  = GradScaler(\"cuda\", enabled=USE_AMP)\n",
    "\n",
    "print(f\"Training config:\")\n",
    "print(f\"  Epochs={EPOCHS}  LR={LR}  WD={WEIGHT_DECAY}\")\n",
    "print(f\"  Warmup={WARMUP_STEPS}  TotalSteps={TOTAL_STEPS:,}\")\n",
    "print(f\"  GradClip={GRAD_CLIP}  Patience={PATIENCE}\")\n",
    "print(f\"  AMP={USE_AMP}  Batch={BATCH_SIZE}  GPUs={n_gpus}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bd52abb-8796-457f-8433-e7b942170ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation utilities ready\n"
     ]
    }
   ],
   "source": [
    "#  14. EVALUATION UTILITIES \n",
    "\n",
    "def compute_metrics(all_logits: np.ndarray, all_labels: np.ndarray):\n",
    "    \"\"\"Hard threshold at logit=0 (sigmoid=0.5).\"\"\"\n",
    "    preds = (all_logits > 0).astype(int)\n",
    "    return {\n",
    "        \"micro_f1\"    : f1_score(all_labels, preds, average=\"micro\",   zero_division=0),\n",
    "        \"macro_f1\"    : f1_score(all_labels, preds, average=\"macro\",   zero_division=0),\n",
    "        \"sample_f1\"   : f1_score(all_labels, preds, average=\"samples\", zero_division=0),\n",
    "        \"micro_prec\"  : precision_score(all_labels, preds, average=\"micro\", zero_division=0),\n",
    "        \"micro_rec\"   : recall_score(all_labels,    preds, average=\"micro\", zero_division=0),\n",
    "        \"per_label_f1\": f1_score(all_labels, preds, average=None, zero_division=0),\n",
    "    }\n",
    "\n",
    "def find_best_thresholds(all_logits: np.ndarray, all_labels: np.ndarray):\n",
    "    \"\"\"\n",
    "    Per-label threshold tuning on validation set.\n",
    "    Sweeps sigmoid probabilities 0.1..0.9 and picks the one maximising F1\n",
    "    for each label independently.\n",
    "    Returns: array of optimal thresholds in logit space.\n",
    "    \"\"\"\n",
    "    probs = 1.0 / (1.0 + np.exp(-all_logits))   # sigmoid\n",
    "    best_thresh = np.zeros(all_labels.shape[1])\n",
    "\n",
    "    for c in range(all_labels.shape[1]):\n",
    "        best_f1, best_t = 0.0, 0.5\n",
    "        for t in np.linspace(0.1, 0.9, 17):\n",
    "            preds_c = (probs[:, c] > t).astype(int)\n",
    "            f1_c    = f1_score(all_labels[:, c], preds_c, zero_division=0)\n",
    "            if f1_c > best_f1:\n",
    "                best_f1, best_t = f1_c, t\n",
    "        best_thresh[c] = best_t\n",
    "\n",
    "    # Convert to logit space for consistency with inference\n",
    "    eps = 1e-7\n",
    "    best_thresh = np.clip(best_thresh, eps, 1 - eps)\n",
    "    return np.log(best_thresh / (1.0 - best_thresh))   # logit thresholds\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader, thresholds=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        thresholds: per-label logit thresholds (array of shape [NUM_LABELS]).\n",
    "                    If None, uses 0.0 (= sigmoid 0.5) for all labels.\n",
    "    \"\"\"\n",
    "    core = model.module if USE_DP else model\n",
    "    core.eval()\n",
    "    all_logits, all_labels_list, total_loss = [], [], 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        ids   = batch[\"chunk_ids\"].to(DEVICE)\n",
    "        masks = batch[\"chunk_masks\"].to(DEVICE)\n",
    "        dm    = batch[\"doc_masks\"].to(DEVICE)\n",
    "        lbls  = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        with autocast(\"cuda\", enabled=USE_AMP):\n",
    "            logits = model(ids, masks, dm) if USE_DP else core(ids, masks, dm)\n",
    "            loss   = criterion(logits, lbls)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_logits.append(logits.cpu().float().numpy())\n",
    "        all_labels_list.append(lbls.cpu().numpy())\n",
    "\n",
    "    all_logits_np = np.concatenate(all_logits, axis=0)\n",
    "    all_labels_np = np.concatenate(all_labels_list, axis=0)\n",
    "\n",
    "    if thresholds is not None:\n",
    "        # Apply per-label thresholds\n",
    "        preds = (all_logits_np > thresholds[None, :]).astype(int)\n",
    "        m = {\n",
    "            \"micro_f1\"    : f1_score(all_labels_np, preds, average=\"micro\",   zero_division=0),\n",
    "            \"macro_f1\"    : f1_score(all_labels_np, preds, average=\"macro\",   zero_division=0),\n",
    "            \"sample_f1\"   : f1_score(all_labels_np, preds, average=\"samples\", zero_division=0),\n",
    "            \"micro_prec\"  : precision_score(all_labels_np, preds, average=\"micro\", zero_division=0),\n",
    "            \"micro_rec\"   : recall_score(all_labels_np,    preds, average=\"micro\", zero_division=0),\n",
    "            \"per_label_f1\": f1_score(all_labels_np, preds, average=None, zero_division=0),\n",
    "        }\n",
    "    else:\n",
    "        m = compute_metrics(all_logits_np, all_labels_np)\n",
    "\n",
    "    m[\"loss\"]       = total_loss / len(loader)\n",
    "    m[\"raw_logits\"] = all_logits_np    # stored for threshold search\n",
    "    m[\"raw_labels\"] = all_labels_np\n",
    "    return m\n",
    "\n",
    "print(\" Evaluation utilities ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef08ea-8f29-47c4-93e4-0bbe9abfb1a4",
   "metadata": {},
   "source": [
    "## Section 5 — Training (Early Stopping, Per-Label Threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d01b12b1-9c87-4b69-8cd8-52ec10a412e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ep01 [ 498/1992] loss=0.8829  lr=7.82e-05\n",
      "  Ep01 [ 996/1992] loss=2.5645  lr=1.56e-04\n",
      "  Ep01 [1494/1992] loss=1.1191  lr=2.00e-04\n",
      "  Ep01 [1992/1992] loss=0.3737  lr=1.99e-04\n",
      "=================================================================\n",
      "  Epoch 01/8  [676s]\n",
      "  train_loss=0.8099  val_loss=0.6211\n",
      "  [default thr]  micro_F1=0.6916  macro_F1=0.6474\n",
      "  [tuned  thr]   micro_F1=0.7355  macro_F1=0.7103  sample_F1=0.7424\n",
      "  P=0.7106  R=0.7622  LR=1.99e-04\n",
      "  💾 Epoch checkpoint saved → epoch_01.pt\n",
      "  ✓ Best checkpoint saved  (micro_F1=0.7355)\n",
      "=================================================================\n",
      "  Ep02 [ 498/1992] loss=0.2201  lr=1.97e-04\n",
      "  Ep02 [ 996/1992] loss=0.0917  lr=1.93e-04\n",
      "  Ep02 [1494/1992] loss=1.0664  lr=1.89e-04\n",
      "  Ep02 [1992/1992] loss=0.1159  lr=1.84e-04\n",
      "=================================================================\n",
      "  Epoch 02/8  [682s]\n",
      "  train_loss=0.5805  val_loss=0.6176\n",
      "  [default thr]  micro_F1=0.7581  macro_F1=0.7373\n",
      "  [tuned  thr]   micro_F1=0.7843  macro_F1=0.7693  sample_F1=0.7934\n",
      "  P=0.7403  R=0.8338  LR=1.84e-04\n",
      "  💾 Epoch checkpoint saved → epoch_02.pt\n",
      "  ✓ Best checkpoint saved  (micro_F1=0.7843)\n",
      "=================================================================\n",
      "  Ep03 [ 498/1992] loss=0.2153  lr=1.77e-04\n",
      "  Ep03 [ 996/1992] loss=0.0613  lr=1.70e-04\n",
      "  Ep03 [1494/1992] loss=0.0148  lr=1.62e-04\n",
      "  Ep03 [1992/1992] loss=0.3027  lr=1.53e-04\n",
      "=================================================================\n",
      "  Epoch 03/8  [684s]\n",
      "  train_loss=0.4757  val_loss=0.7732\n",
      "  [default thr]  micro_F1=0.7677  macro_F1=0.7480\n",
      "  [tuned  thr]   micro_F1=0.7924  macro_F1=0.7771  sample_F1=0.7999\n",
      "  P=0.7747  R=0.8109  LR=1.53e-04\n",
      "  💾 Epoch checkpoint saved → epoch_03.pt\n",
      "  ✓ Best checkpoint saved  (micro_F1=0.7924)\n",
      "=================================================================\n",
      "  Ep04 [ 498/1992] loss=0.2324  lr=1.44e-04\n",
      "  Ep04 [ 996/1992] loss=1.0382  lr=1.34e-04\n",
      "  Ep04 [1494/1992] loss=0.0206  lr=1.24e-04\n",
      "  Ep04 [1992/1992] loss=0.0427  lr=1.14e-04\n",
      "=================================================================\n",
      "  Epoch 04/8  [679s]\n",
      "  train_loss=0.4120  val_loss=0.7607\n",
      "  [default thr]  micro_F1=0.7791  macro_F1=0.7717\n",
      "  [tuned  thr]   micro_F1=0.7919  macro_F1=0.7871  sample_F1=0.7989\n",
      "  P=0.7868  R=0.7970  LR=1.14e-04\n",
      "  💾 Epoch checkpoint saved → epoch_04.pt\n",
      "  No improvement  (1/10)\n",
      "=================================================================\n",
      "  Ep05 [ 498/1992] loss=0.4581  lr=1.03e-04\n",
      "  Ep05 [ 996/1992] loss=0.0538  lr=9.23e-05\n",
      "  Ep05 [1494/1992] loss=0.0228  lr=8.17e-05\n",
      "  Ep05 [1992/1992] loss=0.0139  lr=7.14e-05\n",
      "=================================================================\n",
      "  Epoch 05/8  [685s]\n",
      "  train_loss=0.3365  val_loss=0.8494\n",
      "  [default thr]  micro_F1=0.7811  macro_F1=0.7685\n",
      "  [tuned  thr]   micro_F1=0.7931  macro_F1=0.7852  sample_F1=0.8007\n",
      "  P=0.7679  R=0.8199  LR=7.14e-05\n",
      "  💾 Epoch checkpoint saved → epoch_05.pt\n",
      "  ✓ Best checkpoint saved  (micro_F1=0.7931)\n",
      "=================================================================\n",
      "  Ep06 [ 498/1992] loss=0.0041  lr=6.13e-05\n",
      "  Ep06 [ 996/1992] loss=0.0381  lr=5.17e-05\n",
      "  Ep06 [1494/1992] loss=0.1226  lr=4.27e-05\n",
      "  Ep06 [1992/1992] loss=0.0743  lr=3.43e-05\n",
      "=================================================================\n",
      "  Epoch 06/8  [678s]\n",
      "  train_loss=0.2691  val_loss=0.9242\n",
      "  [default thr]  micro_F1=0.7733  macro_F1=0.7625\n",
      "  [tuned  thr]   micro_F1=0.7928  macro_F1=0.7844  sample_F1=0.7977\n",
      "  P=0.8016  R=0.7841  LR=3.43e-05\n",
      "  💾 Epoch checkpoint saved → epoch_06.pt\n",
      "  No improvement  (1/10)\n",
      "=================================================================\n",
      "  Ep07 [ 498/1992] loss=0.0244  lr=2.66e-05\n",
      "  Ep07 [ 996/1992] loss=0.1146  lr=1.98e-05\n",
      "  Ep07 [1494/1992] loss=0.0200  lr=1.39e-05\n",
      "  Ep07 [1992/1992] loss=0.0086  lr=8.97e-06\n",
      "=================================================================\n",
      "  Epoch 07/8  [677s]\n",
      "  train_loss=0.2277  val_loss=0.9494\n",
      "  [default thr]  micro_F1=0.7724  macro_F1=0.7585\n",
      "  [tuned  thr]   micro_F1=0.7894  macro_F1=0.7789  sample_F1=0.7992\n",
      "  P=0.7898  R=0.7891  LR=8.97e-06\n",
      "  💾 Epoch checkpoint saved → epoch_07.pt\n",
      "  No improvement  (2/10)\n",
      "=================================================================\n",
      "  Ep08 [ 498/1992] loss=0.1139  lr=5.08e-06\n",
      "  Ep08 [ 996/1992] loss=0.8743  lr=2.27e-06\n",
      "  Ep08 [1494/1992] loss=0.0066  lr=5.69e-07\n",
      "  Ep08 [1992/1992] loss=0.0615  lr=0.00e+00\n",
      "=================================================================\n",
      "  Epoch 08/8  [681s]\n",
      "  train_loss=0.1967  val_loss=1.0190\n",
      "  [default thr]  micro_F1=0.7766  macro_F1=0.7628\n",
      "  [tuned  thr]   micro_F1=0.7878  macro_F1=0.7801  sample_F1=0.7992\n",
      "  P=0.7761  R=0.8000  LR=0.00e+00\n",
      "  💾 Epoch checkpoint saved → epoch_08.pt\n",
      "  No improvement  (3/10)\n",
      "=================================================================\n",
      "Training done in 90.8 min\n",
      "Best val micro-F1 (tuned threshold) = 0.7931\n",
      "Per-label optimal thresholds (logit): [ 1.735  2.197  0.847 -0.201 -0.619  0.     2.197]\n"
     ]
    }
   ],
   "source": [
    "#  15. TRAINING LOOP \n",
    "\n",
    "CKPT_DIR = Path(\"./bdh_checkpoints\")\n",
    "CKPT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "history = {k: [] for k in\n",
    "           [\"train_loss\",\"val_loss\",\"val_micro_f1\",\"val_macro_f1\",\"val_sample_f1\",\"lr\"]}\n",
    "\n",
    "best_val_f1      = -1.0\n",
    "best_val_loss    = float(\"inf\")\n",
    "patience_counter = 0\n",
    "best_thresholds  = np.zeros(NUM_LABELS)  # will be updated from val logits\n",
    "t0_total         = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    #  Training \n",
    "    model.train()\n",
    "    epoch_loss, epoch_steps, nan_count = 0.0, 0, 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        ids   = batch[\"chunk_ids\"].to(DEVICE, non_blocking=True)\n",
    "        masks = batch[\"chunk_masks\"].to(DEVICE, non_blocking=True)\n",
    "        dm    = batch[\"doc_masks\"].to(DEVICE, non_blocking=True)\n",
    "        lbls  = batch[\"labels\"].to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(\"cuda\", enabled=USE_AMP):\n",
    "            logits = model(ids, masks, dm)\n",
    "            loss   = criterion(logits, lbls)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            nan_count += 1\n",
    "            print(f\"   NaN at ep{epoch} b{batch_idx}  skipping\")\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            continue\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss  += loss.item()\n",
    "        epoch_steps += 1\n",
    "\n",
    "        log_every = max(1, len(train_loader) // 4)\n",
    "        if (batch_idx + 1) % log_every == 0:\n",
    "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(f\"  Ep{epoch:02d} [{batch_idx+1:>4d}/{len(train_loader)}] \"\n",
    "                  f\"loss={loss.item():.4f}  lr={cur_lr:.2e}\")\n",
    "\n",
    "    avg_loss = epoch_loss / max(1, epoch_steps)\n",
    "\n",
    "    #  Validation (default threshold) \n",
    "    val_m = evaluate(val_loader, thresholds=None)\n",
    "\n",
    "    #  Per-label threshold search on val set \n",
    "    epoch_thresholds = find_best_thresholds(val_m[\"raw_logits\"], val_m[\"raw_labels\"])\n",
    "    val_m_tuned      = evaluate(val_loader, thresholds=epoch_thresholds)\n",
    "\n",
    "    cur_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    history[\"train_loss\"].append(avg_loss)\n",
    "    history[\"val_loss\"].append(val_m[\"loss\"])\n",
    "    history[\"val_micro_f1\"].append(val_m_tuned[\"micro_f1\"])\n",
    "    history[\"val_macro_f1\"].append(val_m_tuned[\"macro_f1\"])\n",
    "    history[\"val_sample_f1\"].append(val_m_tuned[\"sample_f1\"])\n",
    "    history[\"lr\"].append(cur_lr)\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    nan_str  = f\"   {nan_count} NaN batches\" if nan_count else \"\"\n",
    "\n",
    "    print(f\"{'='*65}\")\n",
    "    print(f\"  Epoch {epoch:02d}/{EPOCHS}  [{elapsed:.0f}s]{nan_str}\")\n",
    "    print(f\"  train_loss={avg_loss:.4f}  val_loss={val_m['loss']:.4f}\")\n",
    "    print(f\"  [default thr]  micro_F1={val_m['micro_f1']:.4f}  \"\n",
    "          f\"macro_F1={val_m['macro_f1']:.4f}\")\n",
    "    print(f\"  [tuned  thr]   micro_F1={val_m_tuned['micro_f1']:.4f}  \"\n",
    "          f\"macro_F1={val_m_tuned['macro_f1']:.4f}  \"\n",
    "          f\"sample_F1={val_m_tuned['sample_f1']:.4f}\")\n",
    "    print(f\"  P={val_m_tuned['micro_prec']:.4f}  \"\n",
    "          f\"R={val_m_tuned['micro_rec']:.4f}  LR={cur_lr:.2e}\")\n",
    "\n",
    "    #  Save checkpoint every epoch \n",
    "    epoch_ckpt_path = CKPT_DIR / f\"epoch_{epoch:02d}.pt\"\n",
    "    torch.save({\n",
    "        \"epoch\"        : epoch,\n",
    "        \"model_state\"  : (model.module if USE_DP else model).state_dict(),\n",
    "        \"optim_state\"  : optimizer.state_dict(),\n",
    "        \"sched_state\"  : scheduler.state_dict(),\n",
    "        \"config\"       : CFG,\n",
    "        \"label2idx\"    : label2idx,\n",
    "        \"idx2label\"    : idx2label,\n",
    "        \"top_labels\"   : top_labels,\n",
    "        \"thresholds\"   : epoch_thresholds,\n",
    "        \"val_metrics\"  : val_m_tuned,\n",
    "        \"history\"      : history,\n",
    "    }, epoch_ckpt_path)\n",
    "    print(f\"   Epoch checkpoint saved  {epoch_ckpt_path.name}\")\n",
    "\n",
    "    #  Checkpoint on best tuned micro-F1 \n",
    "    if val_m_tuned[\"micro_f1\"] > best_val_f1:\n",
    "        best_val_f1     = val_m_tuned[\"micro_f1\"]\n",
    "        best_thresholds = epoch_thresholds\n",
    "        torch.save({\n",
    "            \"epoch\"        : epoch,\n",
    "            \"model_state\"  : (model.module if USE_DP else model).state_dict(),\n",
    "            \"optim_state\"  : optimizer.state_dict(),\n",
    "            \"sched_state\"  : scheduler.state_dict(),\n",
    "            \"config\"       : CFG,\n",
    "            \"label2idx\"    : label2idx,\n",
    "            \"idx2label\"    : idx2label,\n",
    "            \"top_labels\"   : top_labels,\n",
    "            \"thresholds\"   : best_thresholds,\n",
    "            \"val_metrics\"  : val_m_tuned,\n",
    "            \"history\"      : history,\n",
    "        }, CKPT_DIR / \"best_model.pt\")\n",
    "        print(f\"   Best checkpoint saved  (micro_F1={best_val_f1:.4f})\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement  ({patience_counter}/{PATIENCE})\")\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\" Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "print(f\"Training done in {(time.time()-t0_total)/60:.1f} min\")\n",
    "print(f\"Best val micro-F1 (tuned threshold) = {best_val_f1:.4f}\")\n",
    "print(f\"Per-label optimal thresholds (logit): {best_thresholds.round(3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_activations_for_dataset(model, dataloader, device, idx2label, top_n=50):\n",
    "    \"\"\"\n",
    "    Run inference on all test cases and extract:\n",
    "    1. Active neuron IDs + activation values per case\n",
    "    2. Build a global neuronconcept co-activation index\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    core = model.module if hasattr(model, 'module') else model\n",
    "    backbone = core.backbone\n",
    "    C = backbone.config\n",
    "    nh, N = C.n_head, C.mlp_internal_dim_multiplier * C.n_embd // C.n_head\n",
    "    \n",
    "    records = []\n",
    "    coactivation = defaultdict(lambda: defaultdict(int))\n",
    "    concept_occurrence = defaultdict(int)\n",
    "    \n",
    "    for batch_idx, (chunk_ids, chunk_masks, doc_masks, labels) in enumerate(dataloader):\n",
    "        chunk_ids   = chunk_ids.to(device)\n",
    "        chunk_masks = chunk_masks.to(device)\n",
    "        doc_masks   = doc_masks.to(device)\n",
    "        \n",
    "        logits = core(chunk_ids, chunk_masks, doc_masks)\n",
    "        probs  = torch.sigmoid(logits).cpu().numpy()\n",
    "        \n",
    "        # Get stored sparse activations from last layer\n",
    "        xy = backbone._last_xy_sparse  # (B_flat, nh, T, N)\n",
    "        B = chunk_ids.shape[0]\n",
    "        n_chunks = chunk_ids.shape[1]\n",
    "        \n",
    "        for b in range(B):\n",
    "            # Pool this sample's activations\n",
    "            start = b * n_chunks\n",
    "            end   = start + n_chunks\n",
    "            sample_xy = xy[start:end].mean(dim=(0, 2)).reshape(-1)  # (16384,)\n",
    "            \n",
    "            active_mask = sample_xy > 0\n",
    "            active_idx  = torch.where(active_mask)[0].cpu().tolist()\n",
    "            active_vals = sample_xy[active_mask].cpu().tolist()\n",
    "            sorted_pairs = sorted(zip(active_idx, active_vals), key=lambda x: -x[1])\n",
    "            \n",
    "            # Get predictions\n",
    "            sample_probs = probs[b]\n",
    "            pred_labels = [idx2label[i] for i in range(len(sample_probs)) \n",
    "                          if sample_probs[i] >= thresholds.get(i, 0.5)]\n",
    "            true_labels = [idx2label[i] for i in range(labels.shape[1]) \n",
    "                          if labels[b, i] > 0.5]\n",
    "            \n",
    "            # === THIS IS WHERE YOU ADD YOUR CONCEPT LABELING ===\n",
    "            # Option A: Use Claude API to label concepts (as you did)\n",
    "            # Option B: Use the concepts_from_claude.txt mapping\n",
    "            # For now, store raw neuron data; concepts added post-hoc\n",
    "            \n",
    "            records.append({\n",
    "                \"case_id\": f\"batch-{batch_idx}-{b}\",\n",
    "                \"violated_articles\": true_labels,\n",
    "                \"predicted_labels\": pred_labels,\n",
    "                \"active_neurons\": [p[0] for p in sorted_pairs[:top_n]],\n",
    "                \"neuron_act_values\": [round(p[1], 6) for p in sorted_pairs[:top_n]],\n",
    "                \"sparsity\": round(len(active_idx) / 16384, 10),\n",
    "            })\n",
    "            \n",
    "            # Build co-activation index:\n",
    "            # For each concept associated with this case's predicted articles,\n",
    "            # record which neurons were active\n",
    "            for nid in active_idx:\n",
    "                for art in pred_labels:\n",
    "                    coactivation[f\"art_{art}_neuron\"][str(nid)] = \\\n",
    "                        coactivation[f\"art_{art}_neuron\"].get(str(nid), 0) + 1\n",
    "    \n",
    "    return records, dict(coactivation)\n",
    "\n",
    "# Run extraction\n",
    "print(\"Extracting activations from test set...\")\n",
    "records, coact = extract_activations_for_dataset(\n",
    "    model, test_loader, DEVICE, idx2label\n",
    ")\n",
    "\n",
    "# Save\n",
    "with open(\"example_records.json\", \"w\") as f:\n",
    "    json.dump(records, f, indent=2)\n",
    "print(f\"Saved {len(records)} records to example_records.json\")\n",
    "\n",
    "# If you already have coactivation_raw.json and concept_occurrence.json\n",
    "# from your concept mapping notebook, those are used directly by serve.py.\n",
    "# Otherwise, save the per-article neuron index:\n",
    "with open(\"coactivation_raw.json\", \"w\") as f:\n",
    "    json.dump(coact, f, indent=2)\n",
    "print(\"Saved coactivation_raw.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eec829-b7d7-4800-8fc3-5591c9ba58f9",
   "metadata": {},
   "source": [
    "## Section 6 — Results & Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b8f2000-6396-40b9-ad1f-5b16ffb435be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./training_curves.png\n"
     ]
    }
   ],
   "source": [
    "#  16. TRAINING CURVES \n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(history[\"train_loss\"], \"o-\", label=\"train\", ms=6)\n",
    "ax.plot(history[\"val_loss\"],   \"s-\", label=\"val\",   ms=6)\n",
    "ax.set_title(\"Loss\"); ax.set_xlabel(\"Epoch\"); ax.legend(); ax.grid(alpha=0.3)\n",
    "\n",
    "# Overfitting gap annotation\n",
    "if len(history[\"train_loss\"]) > 1:\n",
    "    gap = history[\"train_loss\"][-1] - history[\"val_loss\"][-1]\n",
    "    ax.annotate(f\"gap={gap:+.3f}\", xy=(len(history[\"train_loss\"])-1,\n",
    "                history[\"train_loss\"][-1]), fontsize=9, color=\"red\")\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(history[\"val_micro_f1\"],  \"o-\", label=\"micro-F1\",  ms=6)\n",
    "ax.plot(history[\"val_macro_f1\"],  \"s-\", label=\"macro-F1\",  ms=6)\n",
    "ax.plot(history[\"val_sample_f1\"], \"^-\", label=\"sample-F1\", ms=6)\n",
    "ax.set_title(\"Validation F1 (tuned threshold)\")\n",
    "ax.set_xlabel(\"Epoch\"); ax.legend(); ax.grid(alpha=0.3)\n",
    "\n",
    "ax = axes[2]\n",
    "ax.plot(history[\"lr\"], \".-\", color=\"green\")\n",
    "ax.set_title(\"LR Schedule\"); ax.set_xlabel(\"Step\"); ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./training_curves.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved: ./training_curves.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11bedaa1-76c9-402d-b5c8-f99836e89f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article           F1   Precision    Recall   Threshold   Support\n",
      "--------------------------------------------------------------\n",
      "  Art 10        0.8049      0.8250    0.7857       0.850       291\n",
      "  Art 2         0.8174      0.8103    0.8246       0.900       505\n",
      "  Art 3         0.8337      0.8000    0.8705       0.700     1,349\n",
      "  Art 5         0.7903      0.7946    0.7861       0.450     1,368\n",
      "  Art 6         0.7906      0.7493    0.8367       0.350     4,704\n",
      "  Art 8         0.6120      0.5833    0.6437       0.500       710\n",
      "  Art P1-1      0.8472      0.8188    0.8777       0.900     1,421\n",
      "Micro-F1  : 0.7931\n",
      "  Macro-F1  : 0.7852\n",
      "  Sample-F1 : 0.8007\n",
      "  Precision : 0.7679\n",
      "  Recall    : 0.8199\n"
     ]
    }
   ],
   "source": [
    "#  17. PER-LABEL F1 (best checkpoint, tuned thresholds) \n",
    "\n",
    "# Load best checkpoint\n",
    "ckpt = torch.load(CKPT_DIR / \"best_model.pt\", map_location=DEVICE, weights_only=False)\n",
    "core_model = model.module if USE_DP else model\n",
    "core_model.load_state_dict(ckpt[\"model_state\"])\n",
    "best_thresholds = ckpt[\"thresholds\"]\n",
    "\n",
    "val_m = evaluate(val_loader, thresholds=best_thresholds)\n",
    "plf1  = val_m[\"per_label_f1\"]\n",
    "\n",
    "#  Bar chart \n",
    "colors = [\"#d73027\" if f < 0.2 else \"#fc8d59\" if f < 0.4\n",
    "          else \"#fee090\" if f < 0.6 else \"#91bfdb\" if f < 0.8\n",
    "          else \"#4575b4\" for f in plf1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(range(NUM_LABELS), plf1, color=colors)\n",
    "ax.set_xticks(range(NUM_LABELS))\n",
    "ax.set_xticklabels([f\"Art {top_labels[i]}\" for i in range(NUM_LABELS)], fontsize=11)\n",
    "ax.axhline(val_m[\"micro_f1\"], color=\"black\", ls=\"--\", lw=1.5,\n",
    "           label=f\"micro-F1={val_m['micro_f1']:.3f}\")\n",
    "ax.axhline(val_m[\"macro_f1\"], color=\"gray\",  ls=\":\",  lw=1.5,\n",
    "           label=f\"macro-F1={val_m['macro_f1']:.3f}\")\n",
    "for i, v in enumerate(plf1):\n",
    "    ax.text(i, v + 0.02, f\"{v:.2f}\", ha=\"center\", fontsize=10)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_ylabel(\"F1 Score\")\n",
    "ax.set_title(f\"Per-Article F1  Best Checkpoint (Tuned Thresholds)\")\n",
    "ax.legend(); ax.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./per_label_f1.png\", dpi=120, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "#  Table \n",
    "print(f\"{'Article':<12}  {'F1':>6}  {'Precision':>10}  {'Recall':>8}  \"\n",
    "      f\"{'Threshold':>10}  {'Support':>8}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "preds_tuned = (val_m[\"raw_logits\"] > best_thresholds[None, :]).astype(int)\n",
    "for i, lbl in enumerate(top_labels):\n",
    "    p_i   = precision_score(val_m[\"raw_labels\"][:, i], preds_tuned[:, i], zero_division=0)\n",
    "    r_i   = recall_score(val_m[\"raw_labels\"][:, i],    preds_tuned[:, i], zero_division=0)\n",
    "    sup_i = int(label_freq_np[i])\n",
    "    thr_i = 1.0 / (1.0 + np.exp(-best_thresholds[i]))   # to sigmoid probability\n",
    "    print(f\"  Art {lbl:<8s}  {plf1[i]:.4f}  {p_i:>10.4f}  {r_i:>8.4f}  \"\n",
    "          f\"{thr_i:>10.3f}  {sup_i:>8,}\")\n",
    "\n",
    "print(f\"Micro-F1  : {val_m['micro_f1']:.4f}\")\n",
    "print(f\"  Macro-F1  : {val_m['macro_f1']:.4f}\")\n",
    "print(f\"  Sample-F1 : {val_m['sample_f1']:.4f}\")\n",
    "print(f\"  Precision : {val_m['micro_prec']:.4f}\")\n",
    "print(f\"  Recall    : {val_m['micro_rec']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1d35f11-43f8-4746-9f20-037d71475ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set with best thresholds …\n",
      "Test set results (best checkpoint + tuned thresholds):\n",
      "  Loss      = 1.2586\n",
      "  Micro-F1  = 0.7716\n",
      "  Macro-F1  = 0.7675\n",
      "  Sample-F1 = 0.7783\n",
      "  Precision = 0.7643\n",
      "  Recall    = 0.7791\n",
      "Per-label test F1:\n",
      "  Art 10        0.6479\n",
      "  Art 2         0.8850\n",
      "  Art 3         0.8359\n",
      "  Art 5         0.7645\n",
      "  Art 6         0.7709\n",
      "  Art 8         0.6894\n",
      "  Art P1-1      0.7787\n"
     ]
    }
   ],
   "source": [
    "#  18. TEST SET EVALUATION \n",
    "print(\"Evaluating on test set with best thresholds \")\n",
    "test_m = evaluate(test_loader, thresholds=best_thresholds)\n",
    "\n",
    "print(f\"Test set results (best checkpoint + tuned thresholds):\")\n",
    "print(f\"  Loss      = {test_m['loss']:.4f}\")\n",
    "print(f\"  Micro-F1  = {test_m['micro_f1']:.4f}\")\n",
    "print(f\"  Macro-F1  = {test_m['macro_f1']:.4f}\")\n",
    "print(f\"  Sample-F1 = {test_m['sample_f1']:.4f}\")\n",
    "print(f\"  Precision = {test_m['micro_prec']:.4f}\")\n",
    "print(f\"  Recall    = {test_m['micro_rec']:.4f}\")\n",
    "\n",
    "print(f\"Per-label test F1:\")\n",
    "test_plf1 = test_m[\"per_label_f1\"]\n",
    "for i, lbl in enumerate(top_labels):\n",
    "    print(f\"  Art {lbl:<8s}  {test_plf1[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f6eaeb3-b0ad-4acd-913c-2d6d5922c79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files:\n",
      "  ✓ best_model.pt  [245.66 MB]\n",
      "  ✓ training_curves.png  [0.10 MB]\n",
      "  ✓ per_label_f1.png  [0.04 MB]\n",
      "  ✓ label_dist.png  [0.03 MB]\n",
      "✓ All outputs are in /kaggle/working/ — commit the notebook to download them.\n"
     ]
    }
   ],
   "source": [
    "#  20. SAVE ALL OUTPUTS \n",
    "import shutil\n",
    "\n",
    "outputs = [\n",
    "    \"./bdh_checkpoints/best_model.pt\",\n",
    "    \"./training_curves.png\",\n",
    "    \"./per_label_f1.png\",\n",
    "    \"./label_dist.png\",\n",
    "]\n",
    "\n",
    "print(\"Output files:\")\n",
    "for p in outputs:\n",
    "    path = Path(p)\n",
    "    if path.exists():\n",
    "        print(f\"   {path.name}  [{path.stat().st_size/1e6:.2f} MB]\")\n",
    "    else:\n",
    "        print(f\"   {path.name}  (not found)\")\n",
    "\n",
    "print(\" All outputs are in /kaggle/working/  commit the notebook to download them.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac678a-4a6f-4232-9eac-4f2d24f8c0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 15731936,
     "datasetId": 9512642,
     "sourceId": 14869911,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31287,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
